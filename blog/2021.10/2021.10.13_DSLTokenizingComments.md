!!title DSLs Part 4: Tokenizing Comments
!!summary Adding comments to the tokenizer.
!!keywords rust domain specific language parsing structure generation parsing
!!series bloom

## 0619

I've implemented parsing of comments. I'll be utilizing the Lisp comment pattern, where a comment is started by `;` and ended with a new line.

I went with a test driven development approach, where I wrote out my tests then implemented them. The benefit of this is I got something working quickly, then was able to safely refactor it all.

You'll see the final version here. The main change is the addition of a `TokenContext` which will contain all values. I did this as there's several conditions which can trigger the creation of a token. The strangest edge case is a `test; now a comment` which should parse to two tokens, a `test` and a `; now a comment`. 

```
// tokenizer.rs

pub use crate::prelude::*;

/// Context for tokenizing
struct TokenContext<'a> {
    code: &'a str,
    column: usize,
    file: Option<File<'a>>,
    idx: usize,
    is_newline: bool,
    line: usize,
    making_comment: bool,
    start_column: usize,
    terminated: bool,
    token_end_idx: usize,
    token_start_idx: usize,
}

/// Tokenizes the given string.
pub fn tokenize<'a>(code: &'a str, file: Option<File<'a>>) -> Vec<Entity<'a>> {
    let mut entities = vec![];

    let chars_len = code.chars().count();
    let end_idx = if chars_len == 0 { 0 } else { chars_len - 1 };

    let mut ctx = TokenContext {
        code,
        column: 0,
        file,
        idx: 0,
        is_newline: false,
        line: 0,
        making_comment: false,
        start_column: 0,
        terminated: false,
        token_end_idx: 0,
        token_start_idx: 0,
    };

    for c in code.chars() {
        // TODO: handle brackets + parenthesis + curly brackets

        // Check if a comment should be started
        if c == ';' {
            // Attempt to create an entity
            {
                let (new_args, entity) = try_make_token(ctx);
                ctx = new_args;
                if let Some(entity) = entity {
                    entities.push(entity);
                }
            }

            ctx.making_comment = true;
        }

        // Add a new line if necessary
        ctx.is_newline = c == '\n';
        if ctx.is_newline {
            // Terminate comments with new lines.
            if ctx.making_comment {
                ctx.terminated = true;
            }
        } else {
            ctx.column += 1;
        }

        // Update the end index and check if it should be terminated
        if !ctx.terminated {
            let is_end = ctx.idx == end_idx;
            if ctx.making_comment {
                ctx.terminated = is_end;
            } else {
                ctx.terminated = c.is_whitespace() || is_end;
            }
        }

        // Increment end index
        ctx.token_end_idx += 1;

        // Attempt to create an entity if terminated
        if ctx.terminated {
            let (new_args, entity) = try_make_token(ctx);
            ctx = new_args;
            if let Some(entity) = entity {
                entities.push(entity);
            }
        }

        // Increment line + reset column on newlines
        if ctx.is_newline {
            // If needing to handle \r\n specifically, modify code in here.
            // For now it treats \r as whitespace and will break on \n.
            ctx.line += 1;
            ctx.column = 0;
        }

        // Increase index and reset variables
        ctx.idx += 1;
        ctx.terminated = false;
    }

    entities
}

/// Attempts to make a token
fn try_make_token<'a>(mut args: TokenContext<'a>) -> (TokenContext<'a>, Option<Entity>) {
    let mut entity = None;

    // Only push non-whitespace chars
    if !args.code[args.token_start_idx..args.token_end_idx]
        .trim()
        .is_empty()
    {
        let token = if args.making_comment {
            args.making_comment = false;

            make_comment(
                &args.code[args.token_start_idx..args.token_end_idx].trim(),
                args.file,
                args.line,
                args.start_column,
            )
        } else {
            make_token(
                &args.code[args.token_start_idx..args.token_end_idx].trim(),
                args.file,
                args.line,
                args.start_column,
            )
        };

        entity = Some(token);
    }

    args.token_start_idx = args.token_end_idx;
    args.start_column = args.column;

    (args, entity)
}

/// Helper func for making a token
fn make_token<'a>(s: &'a str, file: Option<File<'a>>, ln: usize, col: usize) -> Entity<'a> {
    Entity {
        file,
        is_comment: false,
        location: Location {
            column: col.into(),
            line: ln.into(),
        },
        token: s.into(),
    }
}

/// Helper func for making a comment
fn make_comment<'a>(s: &'a str, file: Option<File<'a>>, ln: usize, col: usize) -> Entity<'a> {
    Entity {
        file,
        is_comment: true,
        location: Location {
            column: col.into(),
            line: ln.into(),
        },
        token: s.into(),
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn make_token_creates_entity() {
        let actual = make_token("wut", Some("WutWut".into()), 1, 2);
        let expected = Entity {
            file: Some("WutWut".into()),
            is_comment: false,
            location: Location {
                line: 1.into(),
                column: 2.into(),
            },
            token: "wut".into(),
        };

        assert_eq!(expected, actual);
    }

    #[test]
    fn make_comment_creates_entity() {
        let actual = make_comment("wut2", Some("WutWutWutWut".into()), 3, 5);
        let expected = Entity {
            file: Some("WutWutWutWut".into()),
            is_comment: true,
            location: Location {
                line: 3.into(),
                column: 5.into(),
            },
            token: "wut2".into(),
        };

        assert_eq!(expected, actual);
    }

    #[test]
    fn one_token() {
        let input = "wut";
        let expected: Vec<Entity<'_>> = vec![make_token("wut", None, 0, 0)];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn one_token_with_starting_spaces() {
        let input = "    wut";
        let expected: Vec<Entity<'_>> = vec![make_token("wut", None, 0, 4)];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn one_token_with_ending_spaces() {
        let input = "wut    ";
        let expected: Vec<Entity<'_>> = vec![make_token("wut", None, 0, 0)];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn two_tokens() {
        let input = "wut up";
        let expected: Vec<Entity<'_>> =
            vec![make_token("wut", None, 0, 0), make_token("up", None, 0, 4)];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn two_tokens_with_spaces() {
        let input = "wut   up";
        let expected: Vec<Entity<'_>> =
            vec![make_token("wut", None, 0, 0), make_token("up", None, 0, 6)];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn two_tokens_with_new_line() {
        let input = "wut \n up";
        let expected: Vec<Entity<'_>> =
            vec![make_token("wut", None, 0, 0), make_token("up", None, 1, 1)];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn two_tokens_with_return_newline() {
        let input = "wut \r\n up";
        let expected: Vec<Entity<'_>> =
            vec![make_token("wut", None, 0, 0), make_token("up", None, 1, 1)];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn many_tokens_with_return_newline() {
        let input = "wut \r\n up    \n     my dawg";
        let expected: Vec<Entity<'_>> = vec![
            make_token("wut", None, 0, 0),
            make_token("up", None, 1, 1),
            make_token("my", None, 2, 5),
            make_token("dawg", None, 2, 8),
        ];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn simple_comment() {
        let input = "; wut";
        let expected: Vec<Entity<'_>> = vec![make_comment("; wut", None, 0, 0)];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn comment_terminated_by_nl() {
        let input = "; wut \n ; wut2";
        let expected: Vec<Entity<'_>> = vec![
            make_comment("; wut", None, 0, 0),
            make_comment("; wut2", None, 1, 1),
        ];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn mixed_comment() {
        let input = "1 + 2 ; wut \n 2 + 1; wut2";
        let expected: Vec<Entity<'_>> = vec![
            make_token("1", None, 0, 0),
            make_token("+", None, 0, 2),
            make_token("2", None, 0, 4),
            make_comment("; wut", None, 0, 6),
            make_token("2", None, 1, 1),
            make_token("+", None, 1, 3),
            make_token("1", None, 1, 5),
            make_comment("; wut2", None, 1, 6),
        ];

        assert_eq!(expected, tokenize(input, None));
    }
}
```

Next session will be adding in brackets, parenthesis and curly braces.