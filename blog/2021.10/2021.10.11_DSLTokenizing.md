!!title DSLs Part 3: Tokenizing
!!summary Implementing a basic tokenizer.
!!keywords rust domain specific language parsing structure generation parsing
!!series bloom

## 0629

I'll be taking a different tack right now and switch over to tokenizing.

One thing I want to try out is a entity driven approach, similar to an ECS in game development. There's a lot of metadata that is attributed to a token or piece of code, so I'll play around with slowly building that up over time.

I'll start by adding a new macro to make types backed by usizes.

```
// macro.rs
macro_rules! make_str_type {
    ($id:ident) => {
        #[derive(Copy, Clone, Debug, PartialEq)]
        pub struct $id<'a>(&'a str);

        impl<'a> From<&'a str> for $id<'a> {
            fn from(s: &'a str) -> Self {
                Self(s)
            }
        }
    };
}

macro_rules! make_usize_type {
    ($id:ident) => {
        #[derive(Copy, Clone, Debug, PartialEq)]
        pub struct $id(usize);

        impl From<usize> for $id {
            fn from(u: usize) -> Self {
                Self(u)
            }
        }
    };
}
```

I'll wire up the various types I'll utilize. These include lines, columns, entities and files.

```
// types.rs
make_str_type!(File);
make_str_type!(Token);

make_usize_type!(Column);
make_usize_type!(Line);

#[derive(Copy, Clone, Debug, PartialEq)]
pub struct Entity<'a> {
    pub file: Option<File<'a>>,
    pub location: Location,
    pub token: Token<'a>,
}

#[derive(Copy, Clone, Debug, PartialEq)]
pub struct Location {
    pub column: Column,
    pub line: Line,
}
```

Finally I'll stub out the method that will parse the code. It's partly implemented with some tests so I can fill in the blanks as time goes on.

```
// tokenizer.rs
pub use crate::prelude::*;

pub fn tokenize<'a>(code: &'a str, file: Option<File<'a>>) -> Vec<Entity<'a>> {
    let mut entities = vec![];

    let mut line = 0;
    let mut column = 0;

    let mut token_start_idx = 0;
    let mut token_end_idx = 0;

    let mut terminated = false;

    for c in code.chars() {
        // TODO: update token start and token end idx
        // TODO: handle comments?
        // TODO: handle brackets?
        // TODO: handle \r\n and \n cases

        if terminated {
            entities.push(make_token(
                &code[token_start_idx..token_end_idx],
                file,
                line,
                column,
            ));

            terminated = false;
        }
    }

    entities
}

/// Helper func for making a token
fn make_token<'a>(s: &'a str, file: Option<File<'a>>, ln: usize, col: usize) -> Entity<'a> {
    Entity {
        file,
        location: Location {
            column: col.into(),
            line: ln.into(),
        },
        token: s.into(),
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn one_token() {
        let input = "wut";
        let expected: Vec<Entity<'_>> = vec![make_token("wut", None, 0, 0)];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn two_tokens() {
        let input = "wut up";
        let expected: Vec<Entity<'_>> =
            vec![make_token("wut", None, 0, 0), make_token("up", None, 0, 4)];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn two_tokens_with_nl() {
        let input = "wut \n up";
        let expected: Vec<Entity<'_>> =
            vec![make_token("wut", None, 0, 0), make_token("up", None, 1, 1)];

        assert_eq!(expected, tokenize(input, None));
    }
}
```

I don't really know what I'll end up using this for, but it will at least give me a launching point for anything I write that seems like it needs to parse code. 

There's two ways I can take this:
1) An approach like Lisp with many parenthesis
2) An approach like Haskell where things are split by whitespace

I'm leaning towards 2 as I generally prefer Haskell like syntax and I think it removes a lot of boilerplate (such as ';').

