!!title Bloom Part 5: Tokenizing Brackets
!!summary Adding brackets, parenthesis and curly braces to the tokenizer.
!!keywords rust domain specific language parsing bloom token tokenizer structure generation parsing
!!series bloom

## 0623

First off I'm renaming the 'DSL' part of this series to 'Bloom'. Why? It was a song off my record collection.

Next up I'm going to be adding in parsing of parenthesis, brackets, and curly braces to the tokenizer.

One hack I'll bring up is running `rm output.txt; cargo test >> output.txt`. What this will do is run a test suite in Rust, save the output to a file. The test output doesn't have line breaks for asserts, which makes it very easy to diff long chains. Extremely useful for comparing tokens with many properties.

## 0644

I've decided to add in [Benchy](https://github.com/ericrobolson/benchy) to this project now. I'll update the `Cargo.toml` file to pull that in:

```
# Cargo.toml
[package]
name = "core_lang"
version = "0.1.0"
edition = "2018"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[features]
default = ["benchmark"]
benchmark = ["benchy/benchmark"]

[dependencies]
benchy = { git = "https://github.com/ericrobolson/benchy"}
```

Next up will be the changes made in this session. I'll wire up `Benchy` to the main loop so when I start compiling I can get an idea of where my hotspots are. 

I'll also add a new `is_terminating_char()` method to check for comments, parenthesis, brackets and curly braces. Finally tests will be added to ensure I'm covering the cases I can think of.

```
// tokenizer.rs
pub use crate::prelude::*;
use benchy::Benchy;

/// Context for tokenizing
struct TokenContext<'a> {
    code: &'a str,
    column: usize,
    file: Option<File<'a>>,
    idx: usize,
    is_newline: bool,
    line: usize,
    making_comment: bool,
    start_column: usize,
    terminated: bool,
    token_end_idx: usize,
    token_start_idx: usize,
}

/// Tokenizes the given string.
pub fn tokenize<'a>(code: &'a str, file: Option<File<'a>>) -> Vec<Entity<'a>> {
    Benchy::time("tokenize()");

    let mut entities = vec![];

    let chars_len = code.chars().count();
    let end_idx = if chars_len == 0 { 0 } else { chars_len - 1 };

    let mut ctx = TokenContext {
        code,
        column: 0,
        file,
        idx: 0,
        is_newline: false,
        line: 0,
        making_comment: false,
        start_column: 0,
        terminated: false,
        token_end_idx: 0,
        token_start_idx: 0,
    };

    for c in code.chars() {
        // Check if a termination happened and if a comment should be started
        if is_terminating_char(c) {
            // Attempt to create an entity
            {
                let (new_args, entity) = try_make_token(ctx);
                ctx = new_args;
                if let Some(entity) = entity {
                    entities.push(entity);
                }
            }

            let is_comment = c == ';';
            if is_comment {
                ctx.making_comment = true;
            } else {
                ctx.terminated = true;
            }
        }

        // Add a new line if necessary
        ctx.is_newline = c == '\n';
        if ctx.is_newline {
            // Terminate comments with new lines.
            if ctx.making_comment {
                ctx.terminated = true;
            }
        } else {
            ctx.column += 1;
        }

        // Update the end index and check if it should be terminated
        if !ctx.terminated {
            let is_end = ctx.idx == end_idx;
            if ctx.making_comment {
                ctx.terminated = is_end;
            } else {
                ctx.terminated = c.is_whitespace() || is_end;
            }
        }

        // Increment end index
        ctx.token_end_idx += 1;

        // Attempt to create an entity if terminated
        if ctx.terminated {
            let (new_args, entity) = try_make_token(ctx);
            ctx = new_args;
            if let Some(entity) = entity {
                entities.push(entity);
            }
        }

        // Increment line + reset column on newlines
        if ctx.is_newline {
            // If needing to handle \r\n specifically, modify code in here.
            // For now it treats \r as whitespace and will break on \n.
            ctx.line += 1;
            ctx.column = 0;
        }

        // Increase index and reset variables
        ctx.idx += 1;
        ctx.terminated = false;
    }

    entities
}

/// Returns whether the given character is a terminating character.
fn is_terminating_char(c: char) -> bool {
    match c {
        ';' => true,
        '(' | ')' => true,
        '[' | ']' => true,
        '{' | '}' => true,
        _ => false,
    }
}

...


#[cfg(test)]
mod tests {
    use super::*;

    ...


    #[test]
    fn tokens_terminated_by_lparen() {
        let input = "wut(wut2";
        let expected: Vec<Entity<'_>> = vec![
            make_token("wut", None, 0, 0),
            make_token("(", None, 0, 3),
            make_token("wut2", None, 0, 4),
        ];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn tokens_terminated_by_rparen() {
        let input = "wut)wut2";
        let expected: Vec<Entity<'_>> = vec![
            make_token("wut", None, 0, 0),
            make_token(")", None, 0, 3),
            make_token("wut2", None, 0, 4),
        ];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn tokens_terminated_by_lbracket() {
        let input = "wut wut2 [";
        let expected: Vec<Entity<'_>> = vec![
            make_token("wut", None, 0, 0),
            make_token("wut2", None, 0, 4),
            make_token("[", None, 0, 9),
        ];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn tokens_terminated_by_rbracket() {
        let input = "]wut wut2 ";
        let expected: Vec<Entity<'_>> = vec![
            make_token("]", None, 0, 0),
            make_token("wut", None, 0, 1),
            make_token("wut2", None, 0, 5),
        ];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn tokens_terminated_by_lcurly_bracket() {
        let input = "wut{ wut2";
        let expected: Vec<Entity<'_>> = vec![
            make_token("wut", None, 0, 0),
            make_token("{", None, 0, 3),
            make_token("wut2", None, 0, 5),
        ];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn tokens_terminated_by_rcurly_bracket() {
        let input = "wut{](){} wut2";
        let expected: Vec<Entity<'_>> = vec![
            make_token("wut", None, 0, 0),
            make_token("{", None, 0, 3),
            make_token("]", None, 0, 4),
            make_token("(", None, 0, 5),
            make_token(")", None, 0, 6),
            make_token("{", None, 0, 7),
            make_token("}", None, 0, 8),
            make_token("wut2", None, 0, 10),
        ];

        assert_eq!(expected, tokenize(input, None));
    }
}
```

With that I think I'm done with the tokenizer. I'll noodle on the parser which will help me convert these tokens (individual strings) into meaningful constructs. Next session I'll start on that.



