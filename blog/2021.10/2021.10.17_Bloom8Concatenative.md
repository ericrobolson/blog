!!title Bloom Part 8: Concatenative Languages
!!summary Making Bloom a concatenative language. 
!!keywords rust domain specific language parsing bloom token tokenizer structure generation parsing concatenative death in chains quest 2
!!series bloom

## 0629

I think I know where I'm going with this project. I really like static types, so I will for sure implement those. I want portability and I want something that is a different paradigm than what I'm used to. A concatenative language fits the bill.

These are languages that have an implicit or explicit stack that gets passed around functions, such as [Joy](http://joy-lang.org/overview-of-joy/) or [Factor](https://factorcode.org/). The idea is that they can be written at a very high level.

I have a few considerations I need to make now. How will I make it easily portable? How will I handle various types? Should they be application or language level constructs?

At the very least, I'll go through and delete code that does not seem relevant.

I deleted that `Reference` type in the parser and refactored the number calculations. The resulting parser looks like so:

```
// parser.rs


#[derive(Copy, Clone, Debug, PartialEq)]
pub enum ParseTree<'a> {
    Bool(Bool<'a>),
    Comment(Comment<'a>),
    Float(Float<'a>),
    Identifier(Identifier<'a>),
    WholeNumber(WholeNumber<'a>),
}

pub fn parse<'a>(mut entities: Vec<Entity<'a>>) -> Vec<Res<ParseTree<'a>>> {
    Benchy::time("core_lang::parser::parse()");

    let mut entity;
    let mut results = vec![];

    // Drain all entities, converting them to a parse tree
    while entities.is_empty() == false {
        entity = entities.remove(0);

        // Bool - false
        if entity.token.as_str() == "false" {
            results.push(ok(make_bool(false, entity)));
            continue;
        }

        // Bool - true
        if entity.token.as_str() == "true" {
            results.push(ok(make_bool(true, entity)));
            continue;
        }

        // Comment
        if entity.is_comment {
            results.push(ok(make_comment(entity)));
            continue;
        }

        // Attempt to make a number
        if let Some(number) = try_make_number(entity) {
            results.push(number);
            continue;
        }

        // TODO: lists and the like

        // Anything else is an identifier
        results.push(ok(make_identifier(entity)));
    }

    results
}


fn try_make_number<'a>(entity: Entity<'a>) -> Option<Res<ParseTree<'a>>> {
    let c = entity.token.as_str().chars().nth(0).unwrap_or(' ');

    match c {
        '-' | '+' | '0' | '1' | '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9' => {
            // We should definitely have either a whole number or a float

            // Number - goes before float due to trying to make the most strict number first
            if let Some(number) = try_make_whole_number(entity) {
                return Some(number);
            }

            // Float
            if let Some(number) = try_make_float(entity) {
                return Some(number);
            }

            // Can safely return none on these
            if c == '+' || c == '-' {
                return None;
            }

            // Unable to parse number, so return error
            return Some(Res::Error {
                entity,
                error: Error::IdentifierStartedWithNumber,
            });
        }
        _ => {}
    }

    None
}
```

I will definitely want strings as well as lists, so I'll work on adding those in now. 

For strings I think I will construct them in the parser. I do want to separate tokens on quotation marks though, so I'll modify my tokenizer for that. I'll also need to handle escaped characters.

The tokenizer changes are pretty simple as I can just add those characters to my `is_terminating_char()` method that splits tokens.

```
// tokenizer.rs

/// Returns whether the given character is a terminating character.
fn is_terminating_char(c: char) -> bool {
    match c {
        ';' => true,
        '(' | ')' => true,
        '[' | ']' => true,
        '{' | '}' => true,
        '"' => true,
        '\\' => true,
        _ => false,
    }
}
```

On second thought I don't think I know how to wire this up in the parser. Like comments, I'll want strings to be handled by the tokenizer.

I'll update my types for a new error and whether an entity is a string or not.

```
// types.rs

make_str_type!(File);
make_str_type!(Token);

make_usize_type!(Column);
make_usize_type!(Line);

#[derive(Copy, Clone, Debug, PartialEq)]
pub struct Entity<'a> {
    pub file: Option<File<'a>>,
    pub is_comment: bool,
    pub is_string: bool,
    pub location: Location,
    pub token: Token<'a>,
}

#[derive(Copy, Clone, Debug, PartialEq)]
pub enum Res<'a, T> {
    Ok { item: T },
    Error { entity: Entity<'a>, error: Error },
    Warn { item: T, warning: Warning },
}

/// A warning that may occur during processing.
#[derive(Copy, Clone, Debug, PartialEq)]
pub enum Warning {}

#[derive(Copy, Clone, Debug, PartialEq)]
pub struct Location {
    pub column: Column,
    pub line: Line,
}

/// An error that occurred.
#[derive(Copy, Clone, Debug, PartialEq)]
pub enum Error {
    IdentifierStartedWithNumber,
    UnclosedString,
}
```

I'll update my `tokenizer` module to create new entity types for strings.

```
// tokenizer.rs

fn make_entity<'a>(
    token: Token<'a>,
    is_comment: bool,
    is_string: bool,
    file: Option<File<'a>>,
    ln: usize,
    col: usize,
) -> Entity<'a> {
    Entity {
        file,
        is_comment,
        is_string,
        location: Location {
            column: col.into(),
            line: ln.into(),
        },
        token,
    }
}

/// Helper func for making a comment
fn make_comment<'a>(s: &'a str, file: Option<File<'a>>, ln: usize, col: usize) -> Entity<'a> {
    make_entity(s.into(), true, false, file, ln, col)
}

/// Helper func for making a token
fn make_token<'a>(s: &'a str, file: Option<File<'a>>, ln: usize, col: usize) -> Entity<'a> {
    make_entity(s.into(), false, false, file, ln, col)
}

/// Helper func for making a string
fn make_string<'a>(s: &'a str, file: Option<File<'a>>, ln: usize, col: usize) -> Entity<'a> {
    make_entity(s.into(), false, true, file, ln, col)
}
```

I don't know how I'm going to implement this. I'm going to have to totally refactor how I'm doing errors, warnings and successes. That will be don in a later session I think. 

Regardless, I can stub out some tests for when I do implement this. 

```
// tokenizer.rs

#[cfg(test)]
mod tests {
    ... 

    #[test]
    fn simple_string() {
        let input = "\"foo bar\"";

        let expected: Vec<Entity<'_>> = vec![make_string("foo bar", None, 0, 0)];
        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn two_strings() {
        let input = "\"foo bar\" \"is da world\"";

        let expected: Vec<Entity<'_>> = vec![
            make_string("foo bar", None, 0, 0),
            make_string("is da world", None, 9, 0),
        ];
        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn string_with_escaped_chars() {
        let input = "\"foo \\\"da\\\" bar\"";

        let expected: Vec<Entity<'_>> = vec![make_string("foo \\\"da\\\" bar", None, 0, 0)];
        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn unclosed_string_returns_error() {
        let input = "\"foo ";
        todo!();
    }

    ...
}
```

They all fail, but that is fine by me. Mainly writing them down so I know what I need to do at a later stage.

Before I go I want to plug In Death Unchained.

![youtube](https://www.youtube.com/embed/icnOsHC0MGs)

I'm really digging the movement as well as the action. There are some areas that could be more polished, but it's a good time. 