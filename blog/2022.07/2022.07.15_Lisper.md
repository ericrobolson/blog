!!title Lisper
!!summary Separating the front end for the compiler
!!keywords rust json parser compiler tokenizer
!!series EggLang

## 0704

I've decided to split off the front end of my compiler into a separate repo. Why? I want to work on some other projects as I've hit a roadblock with EggLang. Functions are proving harder than I had anticipated and I want to work on other projects.

I will certainly circle back to EggLang, but for now I will take what I have and get it into a useable state. [Lisper](https://github.com/ericrobolson/Lisper) is the result. I'll go into a brief overview, but as always analyzing the repo itself will provide the most information.

The design was done in a bottom up fashion, iterating on one test at a time. I knew I was going to need locations of each individual token so I started with that.

```
// location.rs

use std::path::PathBuf;

#[derive(Clone, Debug, PartialEq)]
pub struct Location {
    pub line: usize,
    pub column: usize,
    pub path: PathBuf,
}

impl Location {
    pub fn new(path: PathBuf) -> Self {
        Self {
            line: 0,
            column: 0,
            path,
        }
    }

    pub fn increment_line(mut self) -> Self {
        self.line += 1;
        self
    }
}
```

From there I can build out an `Error` struct that returns the location as well as the type of error that occured.

```
// error.rs

use std::path::PathBuf;

#[derive(Clone, Debug, PartialEq)]
pub struct Location {
    pub line: usize,
    pub column: usize,
    pub path: PathBuf,
}

impl Location {
    pub fn new(path: PathBuf) -> Self {
        Self {
            line: 0,
            column: 0,
            path,
        }
    }

    pub fn increment_line(mut self) -> Self {
        self.line += 1;
        self
    }
}
```

The next task was building out the tokenizer. This is the lowest level part as it will take in a source file and split it up into various tokens. There will be a lot I don't cover, but I'll hit the highlights.

The guts of the tokenizer is a method called `tokenize()`. This takes in some contents and iterates over all of them on a character by character basis, constructing various primitives as it goes along.

```
    /// tokenize the given contents into a series of tokens.
    pub fn tokenize<'a>(contents: &'a str, path: PathBuf) -> Result<Success, Err> {
        let mut tokenizer = Self::load(contents, path);

        let mut prev_char = None;
        while let Some(c) = tokenizer.next_character() {
```

From there I need to do some calculations to drive logic down the line. This involves looking at previous states as well as analyzing the character itself.

```
            let is_comment = c == COMMENT;
            let is_quote = c == QUOTE;
            let is_whitespace = c.is_whitespace();
            let prev_char_is_escape = Some(ESCAPE_CHARACTER) == prev_char;
            let is_newline = c == NEW_LINE;
            let is_symbol = is_symbol(c);
            let is_making_comment = tokenizer.is_making_comment();
            let is_terminal_character = is_symbol | is_whitespace || is_comment || is_newline;
```

From there I will check if we're making a string. Strings are special as quoted quotation marks `\"` signal that a string shouldn't be terminated.

```
            // Handle making a string
            if tokenizer.is_making_string() {
                if is_quote && !prev_char_is_escape {
                    tokenizer.make_string()?;
                } else {
                    let mut state = tokenizer.pop_string_state()?;
                    state.contents.push(c);
                    tokenizer.state_stack.push(State::String(state));
                }
            }
            // End the string
            else if is_quote && !is_making_comment {
                if tokenizer.is_making_identifier() {
                    tokenizer.make_identifier()?;
                }

                tokenizer.state_stack.push(State::String(StringState {
                    start: tokenizer.location.clone(),
                    contents: String::new(),
                }));
```

The next part is checking for terminal characters, such as newlines, comments, symbols and whitespace. These are important as they split tokens.

```
            } else if is_terminal_character {
                if is_whitespace && tokenizer.state_stack.is_empty() {
                    // do nothing
                } else {
                    let mut skip_symbol = false;

                    if tokenizer.is_making_identifier() {
                        tokenizer.make_identifier()?;
                    }

                    if is_comment && !is_making_comment {
                        tokenizer.state_stack.push(State::Comment(CommentState {
                            start: tokenizer.location.clone(),
                            contents: String::new(),
                        }));
                    } else if is_making_comment {
                        if !is_comment {
                            tokenizer.push_char_on_comment(c)?;
                            skip_symbol = true;
                        }
                    }

                    if is_newline && tokenizer.is_making_comment() {
                        tokenizer.make_comment()?;
                    }

                    if is_symbol && !skip_symbol {
                        tokenizer.tokens.push(Token {
                            kind: TokenKind::Symbol(c),
                            location: tokenizer.location.clone(),
                        });
                    }
                }
            } 
```

Finally we check to see if we need to handle comments or identifiers. These will simply start new states or add to existing ones.
```
            else if tokenizer.is_making_identifier() {
                let mut state = tokenizer.pop_identifier_state()?;
                state.contents.push(c);
                tokenizer.state_stack.push(State::Identifier(state));
            } else if tokenizer.is_making_comment() {
                tokenizer.push_char_on_comment(c)?;
            } else {
                // Start identifier
                tokenizer
                    .state_stack
                    .push(State::Identifier(IdentifierState {
                        start: tokenizer.location.clone(),
                        contents: c.to_string(),
                    }));
            }
```

Finally we keep track of the previous character, increment the token location and finalize once done iterating through all characters.

```
            prev_char = Some(c);
            tokenizer.increment_location(c);
        }

        tokenizer.finalize()
    }
```

While there is much more to dig in to, I feel that it is sufficient to get a grasp on how the tokenizer works.

Once that is done the list of tokens is passed in to the parser. Like before, I will only tackle the main `parse()` method. This is far simpler as we simply go through each token and construct a list. Everything in this parser can be represented as any of the following:

```
#[derive(Debug, Clone, PartialEq)]
pub enum Ast {
    List(Vec<Node>),
    Comment(String),
    Identifier(String),
    Number(f64),
    String(String),
}
```

These get wrapped up in a `Node`.

```
#[derive(Debug, Clone, PartialEq)]
pub struct Node {
    pub ast: Ast,
    pub tokens: Vec<Token>,
}
```

The main difference between the tokenizer is that the parser constructs lists and simplifies tokens. Like before, we iterate over all input (tokens in this case) and construct the nodes from that.

```
// parser.rs

 /// Attempts to parse the given tokens into a vec of nodes.
    pub fn parse(tokens: Vec<Token>) -> Result<Vec<Node>, Err> {
        let mut parser = Self::new(tokens);

        while let Some(token) = parser.next_token() {
            match &token.kind {
                TokenKind::Comment(comment) => {
                    let node = Node {
                        ast: Ast::Comment(comment.clone()),
                        tokens: vec![token],
                    };
                    parser.add_node(node)?;
                }
                TokenKind::Number(n) => {
                    let node = Node {
                        ast: Ast::Number(*n),
                        tokens: vec![token],
                    };
                    parser.add_node(node)?;
                }
                TokenKind::Identifier(id) => {
                    let node = Node {
                        ast: Ast::Identifier(id.clone()),
                        tokens: vec![token],
                    };
                    parser.add_node(node)?;
                }
                TokenKind::String(string) => {
                    let node = Node {
                        ast: Ast::String(string.clone()),
                        tokens: vec![token],
                    };
                    parser.add_node(node)?;
                }
                TokenKind::Symbol('(') => {
                    parser.start_list(token);
                }
                TokenKind::Symbol(')') => {
                    parser.end_list(token)?;
                }
                t => todo!("Parse token: {:#?}", t),
            }
        }

        parser.finalize()
    }

```

These are exposed as a `parse()` method in the `lib.rs` file so that users don't have to worry about the internals of this crate.

```
// lib.rs

/// Parse the given contents into a vec of nodes.
pub fn parse<'a>(contents: &'a str, path: std::path::PathBuf) -> Result<Vec<Node>, Error> {
    let tokens = tokenizer::Tokenizer::tokenize(contents, path)?;
    let nodes = parser::Parser::parse(tokens)?;

    Ok(nodes)
}

/// Errors that may occur during parsing.
#[derive(Debug, Clone, PartialEq)]
pub enum Error {
    Tokenizer(tokenizer::Err),
    Parser(parser::Err),
}

impl From<parser::Err> for Error {
    fn from(err: parser::Err) -> Self {
        Self::Parser(err)
    }
}

impl From<tokenizer::Err> for Error {
    fn from(err: tokenizer::Err) -> Self {
        Self::Tokenizer(err)
    }
}
```

While I will be pausing EggLang, the compiler that this code is based off of, I would like to go back in the future and extend it. For now though I have a few app ideas I want to try out and will be iterating on those.

