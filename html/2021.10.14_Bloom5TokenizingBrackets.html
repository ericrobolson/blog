
<!DOCTYPE html>
<html>
    <head>
        <title>Bloom Part 5: Tokenizing Brackets</title>
        <!-- metadata -->
        <meta charset="UTF-8">
<meta name="description" content="Adding brackets, parenthesis and curly braces to the tokenizer.">
<meta name="keywords" content="rust, domain, specific, language, parsing, bloom, token, tokenizer, structure, generation, parsing">
<meta name="author" content="Eric Olson">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
        <!-- css -->
        <link rel="stylesheet" href="assets/styles.css">
    </head>
    <body>
        <!-- page class -->
        <div class=pageLight>
            <!-- content wrapper -->
            <div class=contentWrapperLight>
                <!-- content -->
                <div   ><div   ><div   class="navbottomLight"><a id="previous_page_link" href="2021.10.13_Bloom4TokenizingComments.html"  >ðŸ¡„ Previous</a>
<a id="next_page_link" href="2021.10.14_Bloom6BeginParsing.html"  >Forward ðŸ¡†</a></div>
<div   class="navtopLight"><a id="homeNav" href="index.html"  >Home</a>
<a id="catalogNav" href="catalog.html"  >Catalog</a>
<a id="projectsNav" href="projects.html"  >Projects</a>
<a id="aboutMeNav" href="about_me.html"  >About Me</a></div></div>
<div   ><h2 id="title"  ><a  href="#title"  >#</a>
2021.10.14 - Bloom Part 5: Tokenizing Brackets</h2>
<div   >



<div   class="cardLight hoverShadowLight outlinedLight paddedbottomLight paddedtopLight shadowLight alignmentleftLight"><h3 id="0623"  ><a  href="#0623"  >#</a>
 0623</h3>
<div   ><p   >First off I'm renaming the 'DSL' part of this series to 'Bloom'. Why? It was a song off my record collection.</p>
<p   >Next up I'm going to be adding in parsing of parenthesis, brackets, and curly braces to the tokenizer.</p>
<p   >One hack I'll bring up is running <span   class="codeblockDark outlinedDark">rm output.txt; cargo test >> output.txt</span>. What this will do is run a test suite in Rust, save the output to a file. The test output doesn't have line breaks for asserts, which makes it very easy to diff long chains. Extremely useful for comparing tokens with many properties.</p></div></div>
<div   class="cardLight hoverShadowLight outlinedLight paddedbottomLight paddedtopLight shadowLight alignmentleftLight"><h3 id="0644"  ><a  href="#0644"  >#</a>
 0644</h3>
<div   ><p   >I've decided to add in <a  href="https://github.com/ericrobolson/benchy"  >Benchy</a> to this project now. I'll update the <span   class="codeblockDark outlinedDark">Cargo.toml</span> file to pull that in:</p>
<div   class="codeblockDark contentWrapperLight hoverShadowLight outlinedDark paddedbottomLight paddedtopLight shadowLight">
# Cargo.toml
[package]
name = "core_lang"
version = "0.1.0"
edition = "2018"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[features]
default = ["benchmark"]
benchmark = ["benchy/benchmark"]

[dependencies]
benchy = { git = "https://github.com/ericrobolson/benchy"}
</div>
<p   >Next up will be the changes made in this session. I'll wire up <span   class="codeblockDark outlinedDark">Benchy</span> to the main loop so when I start compiling I can get an idea of where my hotspots are. </p>
<p   >I'll also add a new <span   class="codeblockDark outlinedDark">is_terminating_char()</span> method to check for comments, parenthesis, brackets and curly braces. Finally tests will be added to ensure I'm covering the cases I can think of.</p>
<div   class="codeblockDark contentWrapperLight hoverShadowLight outlinedDark paddedbottomLight paddedtopLight shadowLight">
// tokenizer.rs
pub use crate::prelude::*;
use benchy::Benchy;

/// Context for tokenizing
struct TokenContext&lt;'a> {
    code: &amp;'a str,
    column: usize,
    file: Option&lt;File&lt;'a>>,
    idx: usize,
    is_newline: bool,
    line: usize,
    making_comment: bool,
    start_column: usize,
    terminated: bool,
    token_end_idx: usize,
    token_start_idx: usize,
}

/// Tokenizes the given string.
pub fn tokenize&lt;'a>(code: &amp;'a str, file: Option&lt;File&lt;'a>>) -> Vec&lt;Entity&lt;'a>> {
    Benchy::time("tokenize()");

    let mut entities = vec![];

    let chars_len = code.chars().count();
    let end_idx = if chars_len == 0 { 0 } else { chars_len - 1 };

    let mut ctx = TokenContext {
        code,
        column: 0,
        file,
        idx: 0,
        is_newline: false,
        line: 0,
        making_comment: false,
        start_column: 0,
        terminated: false,
        token_end_idx: 0,
        token_start_idx: 0,
    };

    for c in code.chars() {
        // Check if a termination happened and if a comment should be started
        if is_terminating_char(c) {
            // Attempt to create an entity
            {
                let (new_args, entity) = try_make_token(ctx);
                ctx = new_args;
                if let Some(entity) = entity {
                    entities.push(entity);
                }
            }

            let is_comment = c == ';';
            if is_comment {
                ctx.making_comment = true;
            } else {
                ctx.terminated = true;
            }
        }

        // Add a new line if necessary
        ctx.is_newline = c == '\n';
        if ctx.is_newline {
            // Terminate comments with new lines.
            if ctx.making_comment {
                ctx.terminated = true;
            }
        } else {
            ctx.column += 1;
        }

        // Update the end index and check if it should be terminated
        if !ctx.terminated {
            let is_end = ctx.idx == end_idx;
            if ctx.making_comment {
                ctx.terminated = is_end;
            } else {
                ctx.terminated = c.is_whitespace() || is_end;
            }
        }

        // Increment end index
        ctx.token_end_idx += 1;

        // Attempt to create an entity if terminated
        if ctx.terminated {
            let (new_args, entity) = try_make_token(ctx);
            ctx = new_args;
            if let Some(entity) = entity {
                entities.push(entity);
            }
        }

        // Increment line + reset column on newlines
        if ctx.is_newline {
            // If needing to handle \r\n specifically, modify code in here.
            // For now it treats \r as whitespace and will break on \n.
            ctx.line += 1;
            ctx.column = 0;
        }

        // Increase index and reset variables
        ctx.idx += 1;
        ctx.terminated = false;
    }

    entities
}

/// Returns whether the given character is a terminating character.
fn is_terminating_char(c: char) -> bool {
    match c {
        ';' => true,
        '(' | ')' => true,
        '[' | ']' => true,
        '{' | '}' => true,
        _ => false,
    }
}

...


#[cfg(test)]
mod tests {
    use super::*;

    ...


    #[test]
    fn tokens_terminated_by_lparen() {
        let input = "wut(wut2";
        let expected: Vec&lt;Entity&lt;'_>> = vec![
            make_token("wut", None, 0, 0),
            make_token("(", None, 0, 3),
            make_token("wut2", None, 0, 4),
        ];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn tokens_terminated_by_rparen() {
        let input = "wut)wut2";
        let expected: Vec&lt;Entity&lt;'_>> = vec![
            make_token("wut", None, 0, 0),
            make_token(")", None, 0, 3),
            make_token("wut2", None, 0, 4),
        ];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn tokens_terminated_by_lbracket() {
        let input = "wut wut2 [";
        let expected: Vec&lt;Entity&lt;'_>> = vec![
            make_token("wut", None, 0, 0),
            make_token("wut2", None, 0, 4),
            make_token("[", None, 0, 9),
        ];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn tokens_terminated_by_rbracket() {
        let input = "]wut wut2 ";
        let expected: Vec&lt;Entity&lt;'_>> = vec![
            make_token("]", None, 0, 0),
            make_token("wut", None, 0, 1),
            make_token("wut2", None, 0, 5),
        ];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn tokens_terminated_by_lcurly_bracket() {
        let input = "wut{ wut2";
        let expected: Vec&lt;Entity&lt;'_>> = vec![
            make_token("wut", None, 0, 0),
            make_token("{", None, 0, 3),
            make_token("wut2", None, 0, 5),
        ];

        assert_eq!(expected, tokenize(input, None));
    }

    #[test]
    fn tokens_terminated_by_rcurly_bracket() {
        let input = "wut{](){} wut2";
        let expected: Vec&lt;Entity&lt;'_>> = vec![
            make_token("wut", None, 0, 0),
            make_token("{", None, 0, 3),
            make_token("]", None, 0, 4),
            make_token("(", None, 0, 5),
            make_token(")", None, 0, 6),
            make_token("{", None, 0, 7),
            make_token("}", None, 0, 8),
            make_token("wut2", None, 0, 10),
        ];

        assert_eq!(expected, tokenize(input, None));
    }
}
</div>
<p   >With that I think I'm done with the tokenizer. I'll noodle on the parser which will help me convert these tokens (individual strings) into meaningful constructs. Next session I'll start on that.</p></div></div></div></div></div>
            </div>
        </div>
    </body>
</html>
